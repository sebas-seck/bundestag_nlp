{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ebd843",
   "metadata": {},
   "source": [
    "# External Shock on Energy Politics with new source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595b8846-15bd-48cd-b812-e04bc8973a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import log\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import stylecloud\n",
    "\n",
    "from src.keywords import ANTI_NUCLEAR, CONSERVATIVE_ENERGY, NEUTRAL_ENERGY, PRO_NUCLEAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3feccf",
   "metadata": {},
   "source": [
    "## Energy Politics Keywords\n",
    "The keyword lists from `src/keywords.py` are handcrafted with the help of the topic models in notebook 03. Assignment to a category is guided by these questions:\n",
    "\n",
    "- Is <keyword> helping to leave nuclear energy (in a sustainable manner)? If yes, the topic is **anti nulcear**.\n",
    "- Is <keyword> helping to keep nuclear energy? If yes, the topic is **pro nuclear**.\n",
    "- Is <keyword> associated with conservative energy and does not fit into the above categories? If yes, the topic is **conservative energy**.\n",
    "- Is <keyword> directly associated with energy politics but does not fit into the above categories? If yes, the topic is **neutral energy**.\n",
    "\n",
    "### Stylecloud with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1615140b-f30b-4667-a15a-7ef1d56daf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(ANTI_NUCLEAR + CONSERVATIVE_ENERGY + NEUTRAL_ENERGY + PRO_NUCLEAR)\n",
    "stylecloud.gen_stylecloud(\n",
    "    text=text,\n",
    "    icon_name=\"fas fa-atom\",\n",
    "    palette=\"colorbrewer.qualitative.Dark2_8\",\n",
    "    background_color=\"black\",\n",
    "    gradient=\"horizontal\",\n",
    "    output_name=\"docs/atom.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531177e",
   "metadata": {},
   "source": [
    "![atom wordcloud](docs/atom.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67376176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizes all topic models in a list of lists\n",
    "TOPICS = [ANTI_NUCLEAR, PRO_NUCLEAR, NEUTRAL_ENERGY, CONSERVATIVE_ENERGY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28746efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_extension(topic):\n",
    "    \"\"\"Extends topic models by lemmatized values of existing content\"\"\"\n",
    "    for i in range(0, len(topic)):\n",
    "        doc = gerNLP(topic[i])  # creates the spaCy document per word\n",
    "        if (\n",
    "            doc != doc[0].lemma_\n",
    "        ):  # states condition: lemma has to differ from existing item value\n",
    "            topic.extend(\n",
    "                [doc[0].lemma_]\n",
    "            )  # adds the lemmatized value to the respective topic model list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa59c0-1aa7-4330-81aa-09c208db87f6",
   "metadata": {},
   "source": [
    "### Dataframe Preparation\n",
    "- filter out all speeches not related to energy politics\n",
    "- create a dummy variable whether a speech was given before or after the external shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0616aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gerNLP = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07472325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies topic extension\n",
    "for i in range(0, len(TOPICS)):\n",
    "    topics_extension(TOPICS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f03ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_KEYWORDS = ANTI_NUCLEAR + PRO_NUCLEAR + NEUTRAL_ENERGY + CONSERVATIVE_ENERGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c227cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/open_discourse/speeches.csv\", parse_dates=[\"date\"])\n",
    "df.rename(columns={\"speechContent\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b399964b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899526, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5444bf-b5c0-4552-b6dd-cc0f55d6716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech entries without content 171\n"
     ]
    }
   ],
   "source": [
    "# drops speech entries without content\n",
    "print(\"Speech entries without content\", sum(df[\"text\"].isnull()))\n",
    "df.dropna(subset=[\"text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4242f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 s, sys: 474 ms, total: 45.3 s\n",
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# If executable, this filters all speeches and keeps only those which include at least one word from the hardcoded keywords later in the notebook\n",
    "df = df[df[\"text\"].str.contains(\"|\".join(ALL_KEYWORDS))]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0027bb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40422, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2492364-4f42-4d0a-b343-6e24761667bb",
   "metadata": {},
   "source": [
    "The column *after_shock* is a dummy variable to indicate whether a speech fragment is part of a plenary meeting before the catastrophy in Fukushima or thereafter. The inflection point is between the plenary meetings 97 and 98 during the 17th legislative period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "448c501f-3166-4116-9380-e85d96b2da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"after_shock\"] = np.where(df[\"date\"] < pd.Timestamp(year=2011, month=3, day=11), False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f472e3",
   "metadata": {},
   "source": [
    "### Opinion Analysis Algorithm\n",
    "\n",
    "The class `OpinionAnalyzer()` implements\n",
    "\n",
    "- columns to store keywords and their sentiments as well as numeric scores\n",
    "- counters to track occurances of cases\n",
    "- method `calc_scores` takes a list of words and a list of associated negations and returns a total sentiment score of the list and a list documenting the score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9b2b3",
   "metadata": {},
   "source": [
    "## Algorithm Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814f0989-9c87-4d9b-8c50-3eb485fb75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.opinion_logic import OpinionAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a6b4e-a646-4e67-9720-857a9966c03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 40422 documents parsed\n",
      "100 of 40422 documents parsed\n",
      "200 of 40422 documents parsed\n",
      "300 of 40422 documents parsed\n",
      "400 of 40422 documents parsed\n",
      "500 of 40422 documents parsed\n",
      "600 of 40422 documents parsed\n",
      "700 of 40422 documents parsed\n",
      "800 of 40422 documents parsed\n",
      "900 of 40422 documents parsed\n",
      "1000 of 40422 documents parsed\n",
      "1100 of 40422 documents parsed\n",
      "1200 of 40422 documents parsed\n",
      "1300 of 40422 documents parsed\n",
      "1400 of 40422 documents parsed\n",
      "1500 of 40422 documents parsed\n",
      "1600 of 40422 documents parsed\n",
      "1700 of 40422 documents parsed\n",
      "1800 of 40422 documents parsed\n",
      "1900 of 40422 documents parsed\n",
      "2000 of 40422 documents parsed\n",
      "2100 of 40422 documents parsed\n",
      "2200 of 40422 documents parsed\n",
      "2300 of 40422 documents parsed\n",
      "2400 of 40422 documents parsed\n",
      "2500 of 40422 documents parsed\n",
      "2600 of 40422 documents parsed\n",
      "2700 of 40422 documents parsed\n",
      "2800 of 40422 documents parsed\n",
      "2900 of 40422 documents parsed\n",
      "3000 of 40422 documents parsed\n",
      "3100 of 40422 documents parsed\n",
      "3200 of 40422 documents parsed\n",
      "3300 of 40422 documents parsed\n",
      "3400 of 40422 documents parsed\n",
      "3500 of 40422 documents parsed\n",
      "3600 of 40422 documents parsed\n",
      "3700 of 40422 documents parsed\n",
      "3800 of 40422 documents parsed\n",
      "3900 of 40422 documents parsed\n",
      "4000 of 40422 documents parsed\n",
      "4100 of 40422 documents parsed\n",
      "4200 of 40422 documents parsed\n",
      "4300 of 40422 documents parsed\n",
      "4400 of 40422 documents parsed\n",
      "4500 of 40422 documents parsed\n",
      "4600 of 40422 documents parsed\n",
      "4700 of 40422 documents parsed\n",
      "4800 of 40422 documents parsed\n",
      "4900 of 40422 documents parsed\n",
      "5000 of 40422 documents parsed\n",
      "5100 of 40422 documents parsed\n",
      "5200 of 40422 documents parsed\n",
      "5300 of 40422 documents parsed\n",
      "5400 of 40422 documents parsed\n",
      "5500 of 40422 documents parsed\n",
      "5600 of 40422 documents parsed\n",
      "5700 of 40422 documents parsed\n",
      "5800 of 40422 documents parsed\n",
      "5900 of 40422 documents parsed\n",
      "6000 of 40422 documents parsed\n",
      "6100 of 40422 documents parsed\n",
      "6200 of 40422 documents parsed\n",
      "6300 of 40422 documents parsed\n",
      "6400 of 40422 documents parsed\n",
      "6500 of 40422 documents parsed\n",
      "6600 of 40422 documents parsed\n",
      "6700 of 40422 documents parsed\n",
      "6800 of 40422 documents parsed\n",
      "6900 of 40422 documents parsed\n",
      "7000 of 40422 documents parsed\n",
      "7100 of 40422 documents parsed\n",
      "7200 of 40422 documents parsed\n",
      "7300 of 40422 documents parsed\n",
      "7400 of 40422 documents parsed\n",
      "7500 of 40422 documents parsed\n",
      "7600 of 40422 documents parsed\n",
      "7700 of 40422 documents parsed\n",
      "7800 of 40422 documents parsed\n",
      "7900 of 40422 documents parsed\n",
      "8000 of 40422 documents parsed\n",
      "8100 of 40422 documents parsed\n",
      "8200 of 40422 documents parsed\n",
      "8300 of 40422 documents parsed\n",
      "8400 of 40422 documents parsed\n",
      "8500 of 40422 documents parsed\n",
      "8600 of 40422 documents parsed\n",
      "8700 of 40422 documents parsed\n",
      "8800 of 40422 documents parsed\n",
      "8900 of 40422 documents parsed\n",
      "9000 of 40422 documents parsed\n",
      "9100 of 40422 documents parsed\n",
      "9200 of 40422 documents parsed\n",
      "9300 of 40422 documents parsed\n",
      "9400 of 40422 documents parsed\n",
      "9500 of 40422 documents parsed\n",
      "9600 of 40422 documents parsed\n",
      "9700 of 40422 documents parsed\n",
      "9800 of 40422 documents parsed\n",
      "9900 of 40422 documents parsed\n",
      "10000 of 40422 documents parsed\n",
      "10100 of 40422 documents parsed\n",
      "10200 of 40422 documents parsed\n",
      "10300 of 40422 documents parsed\n",
      "10400 of 40422 documents parsed\n",
      "10500 of 40422 documents parsed\n",
      "10600 of 40422 documents parsed\n",
      "10700 of 40422 documents parsed\n",
      "10800 of 40422 documents parsed\n",
      "10900 of 40422 documents parsed\n",
      "11000 of 40422 documents parsed\n",
      "11100 of 40422 documents parsed\n",
      "11200 of 40422 documents parsed\n",
      "11300 of 40422 documents parsed\n",
      "11400 of 40422 documents parsed\n",
      "11500 of 40422 documents parsed\n",
      "11600 of 40422 documents parsed\n",
      "11700 of 40422 documents parsed\n",
      "11800 of 40422 documents parsed\n",
      "11900 of 40422 documents parsed\n",
      "12000 of 40422 documents parsed\n",
      "12100 of 40422 documents parsed\n",
      "12200 of 40422 documents parsed\n",
      "12300 of 40422 documents parsed\n",
      "12400 of 40422 documents parsed\n",
      "12500 of 40422 documents parsed\n",
      "12600 of 40422 documents parsed\n",
      "12700 of 40422 documents parsed\n",
      "12800 of 40422 documents parsed\n",
      "12900 of 40422 documents parsed\n",
      "13000 of 40422 documents parsed\n",
      "13100 of 40422 documents parsed\n",
      "13200 of 40422 documents parsed\n",
      "13300 of 40422 documents parsed\n",
      "13400 of 40422 documents parsed\n",
      "13500 of 40422 documents parsed\n",
      "13600 of 40422 documents parsed\n",
      "13700 of 40422 documents parsed\n",
      "13800 of 40422 documents parsed\n",
      "13900 of 40422 documents parsed\n",
      "14000 of 40422 documents parsed\n",
      "14100 of 40422 documents parsed\n",
      "14200 of 40422 documents parsed\n",
      "14300 of 40422 documents parsed\n",
      "14400 of 40422 documents parsed\n",
      "14500 of 40422 documents parsed\n",
      "14600 of 40422 documents parsed\n",
      "14700 of 40422 documents parsed\n",
      "14800 of 40422 documents parsed\n",
      "14900 of 40422 documents parsed\n",
      "15000 of 40422 documents parsed\n",
      "15100 of 40422 documents parsed\n",
      "15200 of 40422 documents parsed\n",
      "15300 of 40422 documents parsed\n",
      "15400 of 40422 documents parsed\n",
      "15500 of 40422 documents parsed\n",
      "15600 of 40422 documents parsed\n",
      "15700 of 40422 documents parsed\n",
      "15800 of 40422 documents parsed\n",
      "15900 of 40422 documents parsed\n",
      "16000 of 40422 documents parsed\n",
      "16100 of 40422 documents parsed\n",
      "16200 of 40422 documents parsed\n",
      "16300 of 40422 documents parsed\n",
      "16400 of 40422 documents parsed\n",
      "16500 of 40422 documents parsed\n",
      "16600 of 40422 documents parsed\n",
      "16700 of 40422 documents parsed\n",
      "16800 of 40422 documents parsed\n",
      "16900 of 40422 documents parsed\n",
      "17000 of 40422 documents parsed\n",
      "17100 of 40422 documents parsed\n",
      "17200 of 40422 documents parsed\n",
      "17300 of 40422 documents parsed\n",
      "17400 of 40422 documents parsed\n",
      "17500 of 40422 documents parsed\n",
      "17600 of 40422 documents parsed\n",
      "17700 of 40422 documents parsed\n",
      "17800 of 40422 documents parsed\n",
      "17900 of 40422 documents parsed\n",
      "18000 of 40422 documents parsed\n",
      "18100 of 40422 documents parsed\n",
      "18200 of 40422 documents parsed\n",
      "18300 of 40422 documents parsed\n",
      "18400 of 40422 documents parsed\n",
      "18500 of 40422 documents parsed\n",
      "18600 of 40422 documents parsed\n",
      "18700 of 40422 documents parsed\n",
      "18800 of 40422 documents parsed\n",
      "18900 of 40422 documents parsed\n",
      "19000 of 40422 documents parsed\n",
      "19100 of 40422 documents parsed\n",
      "19200 of 40422 documents parsed\n",
      "19300 of 40422 documents parsed\n",
      "19400 of 40422 documents parsed\n",
      "19500 of 40422 documents parsed\n",
      "19600 of 40422 documents parsed\n",
      "19700 of 40422 documents parsed\n",
      "19800 of 40422 documents parsed\n",
      "19900 of 40422 documents parsed\n",
      "20000 of 40422 documents parsed\n",
      "20100 of 40422 documents parsed\n",
      "20200 of 40422 documents parsed\n",
      "20300 of 40422 documents parsed\n",
      "20400 of 40422 documents parsed\n",
      "20500 of 40422 documents parsed\n",
      "20600 of 40422 documents parsed\n",
      "20700 of 40422 documents parsed\n",
      "20800 of 40422 documents parsed\n",
      "20900 of 40422 documents parsed\n",
      "21000 of 40422 documents parsed\n",
      "21100 of 40422 documents parsed\n",
      "21200 of 40422 documents parsed\n",
      "21300 of 40422 documents parsed\n",
      "21400 of 40422 documents parsed\n",
      "21500 of 40422 documents parsed\n",
      "21600 of 40422 documents parsed\n",
      "21700 of 40422 documents parsed\n",
      "21800 of 40422 documents parsed\n",
      "21900 of 40422 documents parsed\n",
      "22000 of 40422 documents parsed\n",
      "22100 of 40422 documents parsed\n",
      "22200 of 40422 documents parsed\n",
      "22300 of 40422 documents parsed\n",
      "22400 of 40422 documents parsed\n",
      "22500 of 40422 documents parsed\n",
      "22600 of 40422 documents parsed\n",
      "22700 of 40422 documents parsed\n",
      "22800 of 40422 documents parsed\n",
      "22900 of 40422 documents parsed\n",
      "23000 of 40422 documents parsed\n",
      "23100 of 40422 documents parsed\n",
      "23200 of 40422 documents parsed\n",
      "23300 of 40422 documents parsed\n",
      "23400 of 40422 documents parsed\n",
      "23500 of 40422 documents parsed\n",
      "23600 of 40422 documents parsed\n",
      "23700 of 40422 documents parsed\n",
      "23800 of 40422 documents parsed\n",
      "23900 of 40422 documents parsed\n",
      "24000 of 40422 documents parsed\n",
      "24100 of 40422 documents parsed\n",
      "24200 of 40422 documents parsed\n",
      "24300 of 40422 documents parsed\n",
      "24400 of 40422 documents parsed\n",
      "24500 of 40422 documents parsed\n",
      "24600 of 40422 documents parsed\n",
      "24700 of 40422 documents parsed\n",
      "24800 of 40422 documents parsed\n",
      "24900 of 40422 documents parsed\n",
      "25000 of 40422 documents parsed\n",
      "25100 of 40422 documents parsed\n",
      "25200 of 40422 documents parsed\n",
      "25300 of 40422 documents parsed\n",
      "25400 of 40422 documents parsed\n",
      "25500 of 40422 documents parsed\n",
      "25600 of 40422 documents parsed\n",
      "25700 of 40422 documents parsed\n",
      "25800 of 40422 documents parsed\n",
      "25900 of 40422 documents parsed\n",
      "26000 of 40422 documents parsed\n",
      "26100 of 40422 documents parsed\n",
      "26200 of 40422 documents parsed\n",
      "26300 of 40422 documents parsed\n",
      "26400 of 40422 documents parsed\n",
      "26500 of 40422 documents parsed\n",
      "26600 of 40422 documents parsed\n",
      "26700 of 40422 documents parsed\n",
      "26800 of 40422 documents parsed\n",
      "26900 of 40422 documents parsed\n",
      "27000 of 40422 documents parsed\n",
      "27100 of 40422 documents parsed\n",
      "27200 of 40422 documents parsed\n",
      "27300 of 40422 documents parsed\n",
      "27400 of 40422 documents parsed\n",
      "27500 of 40422 documents parsed\n",
      "27600 of 40422 documents parsed\n",
      "27700 of 40422 documents parsed\n",
      "27800 of 40422 documents parsed\n",
      "27900 of 40422 documents parsed\n",
      "28000 of 40422 documents parsed\n",
      "28100 of 40422 documents parsed\n",
      "28200 of 40422 documents parsed\n",
      "28300 of 40422 documents parsed\n",
      "28400 of 40422 documents parsed\n",
      "28500 of 40422 documents parsed\n",
      "28600 of 40422 documents parsed\n",
      "28700 of 40422 documents parsed\n",
      "28800 of 40422 documents parsed\n",
      "28900 of 40422 documents parsed\n",
      "29000 of 40422 documents parsed\n",
      "29100 of 40422 documents parsed\n",
      "29200 of 40422 documents parsed\n",
      "29300 of 40422 documents parsed\n",
      "29400 of 40422 documents parsed\n",
      "29500 of 40422 documents parsed\n",
      "29600 of 40422 documents parsed\n",
      "29700 of 40422 documents parsed\n",
      "29800 of 40422 documents parsed\n",
      "29900 of 40422 documents parsed\n",
      "30000 of 40422 documents parsed\n",
      "30100 of 40422 documents parsed\n",
      "30200 of 40422 documents parsed\n",
      "30300 of 40422 documents parsed\n",
      "30400 of 40422 documents parsed\n",
      "30500 of 40422 documents parsed\n",
      "30600 of 40422 documents parsed\n",
      "30700 of 40422 documents parsed\n",
      "30800 of 40422 documents parsed\n",
      "30900 of 40422 documents parsed\n",
      "31000 of 40422 documents parsed\n",
      "31100 of 40422 documents parsed\n",
      "31200 of 40422 documents parsed\n",
      "31300 of 40422 documents parsed\n",
      "31400 of 40422 documents parsed\n",
      "31500 of 40422 documents parsed\n",
      "31600 of 40422 documents parsed\n",
      "31700 of 40422 documents parsed\n",
      "31800 of 40422 documents parsed\n",
      "31900 of 40422 documents parsed\n",
      "32000 of 40422 documents parsed\n",
      "32100 of 40422 documents parsed\n",
      "32200 of 40422 documents parsed\n",
      "32300 of 40422 documents parsed\n",
      "32400 of 40422 documents parsed\n",
      "32500 of 40422 documents parsed\n",
      "32600 of 40422 documents parsed\n",
      "32700 of 40422 documents parsed\n",
      "32800 of 40422 documents parsed\n",
      "32900 of 40422 documents parsed\n",
      "33000 of 40422 documents parsed\n",
      "33100 of 40422 documents parsed\n",
      "33200 of 40422 documents parsed\n",
      "33300 of 40422 documents parsed\n",
      "33400 of 40422 documents parsed\n",
      "33500 of 40422 documents parsed\n",
      "33600 of 40422 documents parsed\n",
      "33700 of 40422 documents parsed\n",
      "33800 of 40422 documents parsed\n",
      "33900 of 40422 documents parsed\n",
      "34000 of 40422 documents parsed\n",
      "34100 of 40422 documents parsed\n",
      "34200 of 40422 documents parsed\n",
      "34300 of 40422 documents parsed\n",
      "34400 of 40422 documents parsed\n",
      "34500 of 40422 documents parsed\n",
      "34600 of 40422 documents parsed\n",
      "34700 of 40422 documents parsed\n",
      "34800 of 40422 documents parsed\n",
      "34900 of 40422 documents parsed\n",
      "35000 of 40422 documents parsed\n",
      "35100 of 40422 documents parsed\n",
      "35200 of 40422 documents parsed\n",
      "35300 of 40422 documents parsed\n",
      "35400 of 40422 documents parsed\n",
      "35500 of 40422 documents parsed\n",
      "35600 of 40422 documents parsed\n",
      "35700 of 40422 documents parsed\n",
      "35800 of 40422 documents parsed\n",
      "35900 of 40422 documents parsed\n",
      "36000 of 40422 documents parsed\n",
      "36100 of 40422 documents parsed\n",
      "36200 of 40422 documents parsed\n",
      "36300 of 40422 documents parsed\n",
      "36400 of 40422 documents parsed\n",
      "36500 of 40422 documents parsed\n",
      "36600 of 40422 documents parsed\n",
      "36700 of 40422 documents parsed\n",
      "36800 of 40422 documents parsed\n",
      "36900 of 40422 documents parsed\n",
      "37000 of 40422 documents parsed\n",
      "37100 of 40422 documents parsed\n",
      "37200 of 40422 documents parsed\n",
      "37300 of 40422 documents parsed\n",
      "37400 of 40422 documents parsed\n",
      "37500 of 40422 documents parsed\n",
      "37600 of 40422 documents parsed\n",
      "37700 of 40422 documents parsed\n",
      "37800 of 40422 documents parsed\n",
      "37900 of 40422 documents parsed\n",
      "38000 of 40422 documents parsed\n",
      "38100 of 40422 documents parsed\n",
      "38200 of 40422 documents parsed\n",
      "38300 of 40422 documents parsed\n",
      "38400 of 40422 documents parsed\n",
      "38500 of 40422 documents parsed\n",
      "38600 of 40422 documents parsed\n",
      "38700 of 40422 documents parsed\n",
      "38800 of 40422 documents parsed\n",
      "38900 of 40422 documents parsed\n",
      "39000 of 40422 documents parsed\n",
      "39100 of 40422 documents parsed\n",
      "39200 of 40422 documents parsed\n",
      "39300 of 40422 documents parsed\n",
      "39400 of 40422 documents parsed\n",
      "39500 of 40422 documents parsed\n",
      "39600 of 40422 documents parsed\n",
      "39700 of 40422 documents parsed\n",
      "39800 of 40422 documents parsed\n",
      "39900 of 40422 documents parsed\n",
      "40000 of 40422 documents parsed\n",
      "40100 of 40422 documents parsed\n",
      "40200 of 40422 documents parsed\n",
      "40300 of 40422 documents parsed\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "opinion = OpinionAnalyzer(df)\n",
    "df = opinion.main()\n",
    "opinion.protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78011042-2220-4501-970d-89543e8d9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_new_source.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef15821",
   "metadata": {},
   "source": [
    "### Score Splitting at Inflection Point\n",
    "As it is essential to compare opinions before and after the external shock, both values extracted from the calculated scores in the various categories in conjunction with the dummy variable *after_shock*, which is 0 for pre-shock and 1 for after-shock speech fragments. <br>\n",
    "To do so, pre-shock and after-shock columns for each category are created (i.e. *NE_sp*, *PN_sa*). These are children to the main category score columns (i.e. *NE_s*, *PN_s*). Initially, the children columns take on the same value as the parents. Then, the values are adjusted to 0 if the *after_shock* variable value (0 or 1) does not match with the children score column suffices (pre or after). Accordingly, for all rows with a *after_shock* value of 1, the pre score will be reset to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381961d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets all children score columns to equal the parents' values\n",
    "df[\"AN_sp\"] = df[\"AN_s\"]\n",
    "df[\"AN_sa\"] = df[\"AN_s\"]\n",
    "\n",
    "df[\"PN_sp\"] = df[\"PN_s\"]\n",
    "df[\"PN_sa\"] = df[\"PN_s\"]\n",
    "\n",
    "df[\"NE_sp\"] = df[\"NE_s\"]\n",
    "df[\"NE_sa\"] = df[\"NE_s\"]\n",
    "\n",
    "df[\"CE_sp\"] = df[\"CE_s\"]\n",
    "df[\"CE_sa\"] = df[\"CE_s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ff0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusts children score columns for their after_shock values\n",
    "df.loc[df.after_shock == 1, [\"AN_sp\", \"PN_sp\", \"NE_sp\", \"CE_sp\"]] = 0\n",
    "df.loc[df.after_shock == 0, [\"AN_sa\", \"PN_sa\", \"NE_sa\", \"CE_sa\"]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53378def",
   "metadata": {},
   "source": [
    "### Score Calculation\n",
    "The main score is calculated using three of the four keyword lists. The score is designed to be positive to reflect progressiveness. Therefore, anti-nuclear energy opinions are added, pro-nuclear energy opinions are substracted, and conservative energy politics opinions are substracted as well, as those do not reflect the turnaround performed by politics. Solely opinions about energy politics which do not fall into any of the other three categories are not included in the score but are kept for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"] = df[\"AN_s\"] - df[\"PN_s\"] - df[\"CE_s\"]\n",
    "df[\"score_p\"] = df[\"AN_sp\"] - df[\"PN_sp\"] - df[\"CE_sp\"]\n",
    "df[\"score_a\"] = df[\"AN_sa\"] - df[\"PN_sa\"] - df[\"CE_sa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e9225",
   "metadata": {},
   "source": [
    "### Delay Weight\n",
    "#### Weight Calculation\n",
    "The delay weight assigns a value between $1$ and $\\log_{log\\_base}(\\infty)$ to each row. Later, the score will be divided by the weight to calculate the tenacity. The later a speech was given after the external shock, the less impact its score has. The delay weight will be useful in the calculation of the measure of tenacity.\n",
    "<br>\n",
    "The column *delay* is supposed to indicate how many sessions after the external shock a speech has been given. Values for speeches given before the external shock will be overwritten in the end as they are negative and will all be set to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a3c0a-2329-409e-b8ba-bb0dfbf07b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes the delay column to be similar to the running session number\n",
    "df[\"delay\"] = df[\"sitzung\"]\n",
    "# the auxiliary column stores an adjustment value which will be added to delay\n",
    "# for speeches given in legislative period 17, 94 is substracted as the external shock took place inbetween the plenary meetings 94 and 95\n",
    "df.loc[df.electoralTerm <= 17, \"aux_delay\"] = -94\n",
    "# for speeches given in legislative period 18, 159 is added as the external shock took place 159 plenary meetings before the first meeting in legislative period 18\n",
    "df.loc[df.electoralTerm >= 18, \"aux_delay\"] = 253 - 94\n",
    "# the new variable reflects the adjustments from the chunk above\n",
    "df[\"aux_delay_weight\"] = df[\"delay\"] + df[\"aux_delay\"]\n",
    "# sets the variable log_base\n",
    "log_base = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d47e2e",
   "metadata": {},
   "source": [
    "The *log_base* variable is an important argument to the delay weight. Instead of using the delay in number of sessions to punish late speeches / opinion changes, I am choosing to apply a non-linear scale which does not completely invalidate late speeches. Further, it even promotes (attaches more weight to a speech compared to speeches given before the external shock) early speeches just after the external shock up to the point where $aux\\_delay\\_weight = log\\_base$. Thus, choosing $log\\_base = 3$ would\n",
    "\n",
    "As I do not want to punish any speech given before the external shock, the  auxiliary variable *aux_delay_weight* is set to equal the *log_base* value for all speeches before the shock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.after_shock == 0, \"aux_delay_weight\"] = log_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af66db4",
   "metadata": {},
   "source": [
    "In the next chunk, the non-linear weighting value is completed. As all speeches before the shock have an *aux_delay_weight* equal to *log_base*, the final weight *delay_weight* will be $log_{log\\_base}(log\\_base) = 1$.\n",
    "Accordingly, more importance is attached to speeches in the first two plenary meetings after the external shock. A weight of 1 would be attached to all plenary meetings before the external shock and plenary meeting 3 after the shock. All plenary meetings after number 3 would receive a slowly decreasing importance. For example, meeting 200 after the shock would receive a weight of $\\frac{1}{log_3 200}=\\frac{1}{4.8}$ - the opinions voiced at that time are roughly a fifth as important as an opinion voiced in plenary meeting 3 after the shock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"delay_weight\"] = df[\"aux_delay_weight\"].apply(lambda x: log(x, log_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce404ca",
   "metadata": {},
   "source": [
    "Lastly, all auxialilary columns no longer needed can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"delay\"]\n",
    "del df[\"aux_delay\"]\n",
    "del df[\"aux_delay_weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf7e43",
   "metadata": {},
   "source": [
    "#### Application of Delay Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe69a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"w_score\"] = df[\"score\"] / df[\"delay_weight\"]\n",
    "df[\"w_score_p\"] = df[\"score_p\"] / df[\"delay_weight\"]\n",
    "df[\"w_score_a\"] = df[\"score_a\"] / df[\"delay_weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ab66e",
   "metadata": {},
   "source": [
    "### Review of Preliminary Results\n",
    "The next chunk returns a sample of rows with strong positive and negative scores. These speech fragments are examples of extremely strong opinions towards nuclear energy politics being voiced in parliament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5635bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\n",
    "    \"display.max_colwidth\",\n",
    "    25,\n",
    "    \"display.precision\",\n",
    "    2,\n",
    "    \"display.float_format\",\n",
    "    lambda x: \"%.2f\" % x,\n",
    "):\n",
    "    print(\n",
    "        df[\n",
    "            [\n",
    "                \"PN_s\",\n",
    "                \"PN_sp\",\n",
    "                \"PN_sa\",\n",
    "                \"AN_s\",\n",
    "                \"AN_sp\",\n",
    "                \"AN_sa\",\n",
    "                \"CE_s\",\n",
    "                \"CE_sp\",\n",
    "                \"CE_sa\",\n",
    "                \"NE_s\",\n",
    "                \"NE_sp\",\n",
    "                \"NE_sa\",\n",
    "                \"score\",\n",
    "                \"score_p\",\n",
    "                \"score_a\",\n",
    "                \"w_score\",\n",
    "                \"w_score_p\",\n",
    "                \"w_score_a\",\n",
    "                \"delay_weight\",\n",
    "            ]\n",
    "        ].describe()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a027835-4e48-4ab9-ba79-93bed581d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"df_new_source.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72ddb2-6dae-4c60-9402-bafce12ba114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
